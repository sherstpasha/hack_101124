{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется устройство: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\pasha/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2024-11-10 Python-3.11.0 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 4060, 8188MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 224 layers, 7167184 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "Using cache found in C:\\Users\\pasha/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2024-11-10 Python-3.11.0 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 4060, 8188MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Видео 1.mp4 длительностью 17.33 секунд, FPS: 30.0\n",
      "Сегмент 2/2, время: 10.00-17.33 сек, Класс: Статья 12.16. часть 1 Несоблюдение требований, предписанных дорожными знаками или разметкой проезжей части дороги, Доверие: 0.2734, Время нарушения: 13 сек\n",
      "Видео 10.mp4 длительностью 60.07 секунд, FPS: 30.0\n",
      "Видео 3.mp4 длительностью 40.03 секунд, FPS: 30.0\n",
      "Видео 4.mp4 длительностью 28.90 секунд, FPS: 29.97002997002997\n",
      "Сегмент 1/3, время: 0.00-10.00 сек, Класс: Статья 12.16. часть 1 Несоблюдение требований, предписанных дорожными знаками или разметкой проезжей части дороги, Доверие: 0.3591, Время нарушения: 5 сек\n",
      "Сегмент 2/3, время: 10.00-20.00 сек, Класс: Статья 12.16. часть 1 Несоблюдение требований, предписанных дорожными знаками или разметкой проезжей части дороги, Доверие: 0.3371, Время нарушения: 15 сек\n",
      "Сегмент 3/3, время: 20.00-28.90 сек, Класс: Статья 12.16. часть 1 Несоблюдение требований, предписанных дорожными знаками или разметкой проезжей части дороги, Доверие: 0.4242, Время нарушения: 24 сек\n",
      "Видео 6.mp4 длительностью 40.85 секунд, FPS: 20.0\n",
      "Видео 7.mp4 длительностью 31.83 секунд, FPS: 30.0\n",
      "Сегмент 2/4, время: 10.00-20.00 сек, Класс: Статья 12.16. часть 1 Несоблюдение требований, предписанных дорожными знаками или разметкой проезжей части дороги, Доверие: 0.2921, Время нарушения: 15 сек\n",
      "Сегмент 4/4, время: 30.00-31.83 сек, Класс: Статья 12.16. часть 1 Несоблюдение требований, предписанных дорожными знаками или разметкой проезжей части дороги, Доверие: 0.3316, Время нарушения: 30 сек\n",
      "Видео 8.mp4 длительностью 19.52 секунд, FPS: 29.97002997002997\n",
      "Видео 9.mp4 длительностью 17.08 секунд, FPS: 25.0\n",
      "Видео akn00006_fqGg6dtL.mov длительностью 300.03 секунд, FPS: 29.97002997002997\n",
      "Сегмент 8/31, время: 70.00-80.00 сек, Класс: Статья 12.16. часть 1 Несоблюдение требований, предписанных дорожными знаками или разметкой проезжей части дороги, Доверие: 0.3666, Время нарушения: 75 сек\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 311\u001b[0m\n\u001b[0;32m    304\u001b[0m video_files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    305\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(video_folder, filename)\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(video_folder)\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mov\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.avi\u001b[39m\u001b[38;5;124m'\u001b[39m))  \u001b[38;5;66;03m# Замените на нужные расширения файлов\u001b[39;00m\n\u001b[0;32m    308\u001b[0m ]\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# Обработка видео и создание сабмита\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m \u001b[43mcreate_submission\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_csv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessor\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 293\u001b[0m, in \u001b[0;36mcreate_submission\u001b[1;34m(video_paths, output_csv, preprocessor)\u001b[0m\n\u001b[0;32m    290\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_path \u001b[38;5;129;01min\u001b[39;00m video_paths:\n\u001b[1;32m--> 293\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m     all_predictions\u001b[38;5;241m.\u001b[39mextend(predictions)\n\u001b[0;32m    296\u001b[0m submission_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_predictions)\n",
      "Cell \u001b[1;32mIn[28], line 261\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(video_path, preprocessor)\u001b[0m\n\u001b[0;32m    258\u001b[0m start_time \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m*\u001b[39m segment_duration\n\u001b[0;32m    259\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m((i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m segment_duration, duration)\n\u001b[1;32m--> 261\u001b[0m predicted_class, confidence \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_segment_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_capture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessor\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# Вычисляем среднее время сегмента\u001b[39;00m\n\u001b[0;32m    266\u001b[0m violation_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((start_time \u001b[38;5;241m+\u001b[39m end_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[28], line 206\u001b[0m, in \u001b[0;36mpredict_segment_class\u001b[1;34m(video_capture, start_time, end_time, preprocessor)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_segment_class\u001b[39m(video_capture, start_time, end_time, preprocessor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;66;03m# Извлекаем кадры из сегмента\u001b[39;00m\n\u001b[1;32m--> 206\u001b[0m     video_frames_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mextract_frames_from_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo_capture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessor\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     video_frames_tensor \u001b[38;5;241m=\u001b[39m video_frames_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Добавляем размерность batch\u001b[39;00m\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;66;03m# Генерируем фиктивный текстовый ввод\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[28], line 185\u001b[0m, in \u001b[0;36mextract_frames_from_video\u001b[1;34m(video_capture, start_time, end_time, num_frames, preprocessor)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Применение предварительной обработки\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 185\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m frame_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m    188\u001b[0m frame_pil \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(frame_rgb)\n",
      "Cell \u001b[1;32mIn[28], line 122\u001b[0m, in \u001b[0;36mPreprocessor.apply\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    116\u001b[0m results_combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[0;32m    117\u001b[0m     [results_pretrained\u001b[38;5;241m.\u001b[39mpandas()\u001b[38;5;241m.\u001b[39mxyxy[\u001b[38;5;241m0\u001b[39m], results_custom\u001b[38;5;241m.\u001b[39mpandas()\u001b[38;5;241m.\u001b[39mxyxy[\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    118\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    119\u001b[0m )\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Обработка кадра моделью SegFormer\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m seg_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_segformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Создание маски для затемнения\u001b[39;00m\n\u001b[0;32m    125\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((height, width), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n",
      "Cell \u001b[1;32mIn[28], line 157\u001b[0m, in \u001b[0;36mPreprocessor.predict_segformer\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    155\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits  \u001b[38;5;66;03m# [batch_size, num_classes, height, width]\u001b[39;00m\n\u001b[0;32m    156\u001b[0m segmentation \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msegmentation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from transformers import (\n",
    "    XCLIPModel,\n",
    "    XCLIPProcessor,\n",
    "    SegformerForSemanticSegmentation,\n",
    "    SegformerImageProcessor,\n",
    ")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model import EmbeddingClassifier\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Игнорировать все предупреждения\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ======== Глобальные параметры ======== #\n",
    "video_folder = r\"C:\\Users\\pasha\\OneDrive\\Рабочий стол\\dataset1011\\videos\"  # Папка с видео\n",
    "output_csv = r'submission.csv'\n",
    "MODEL_NAME = \"microsoft/xclip-base-patch16\"  # Не менять\n",
    "\n",
    "YOLO_CUSTOM_PATH = r\"C:\\Users\\pasha\\OneDrive\\Рабочий стол\\best_93.pt\"  # Путь к кастомной модели YOLO\n",
    "SEGFORMER_MODEL_PATH = r\"C:\\Users\\pasha\\OneDrive\\Рабочий стол\\model\"    # Путь к модели SegFormer\n",
    "BEST_MODEL_PATH = 'best_model_dataset_1_47.pth'  # Обученная модель классификатора\n",
    "\n",
    "APPLY_PREPROCESSING = True  # Переключатель для применения предварительной обработки\n",
    "\n",
    "# ======== Список меток и штрафов ======== #\n",
    "LABEL_LIST = ['Статья 12.16. часть 1 Несоблюдение требований, предписанных дорожными знаками или разметкой проезжей части дороги',\n",
    "                 'нарушений нет',\n",
    "               'Статья 12.16 часть 2 Поворот налево или разворот в нарушение требований, предписанных дорожными знаками или разметкой проезжей части дороги',\n",
    "                   'Статья 12.17  часть 1.1 и 1.2. движение транспортных средств по полосе для маршрутных транспортных средств или остановка на указанной полосе в нарушение Правил дорожного движения ',\n",
    "                     'Статья 12.12 часть 2 1. невыполнение требования ПДД об остановке перед стоп-линией, обозначенной дорожными знаками или разметкой проезжей части дороги, при запрещающем сигнале светофора или запрещающем жесте регулировщика',\n",
    "                       'Статья 12.15 часть 4 Выезд в нарушение правил дорожного движения на полосу, предназначенную для встречного движения, при объезде препятствия, либо на трамвайные пути встречного направления, за исключением случаев, предусмотренных частью 3 настоящей статьи']\n",
    "NUM_CLASSES = len(LABEL_LIST)\n",
    "\n",
    "FINE_DICT = {\n",
    "    'нарушений нет': 0,\n",
    "    'Статья 12.16. часть 1 Несоблюдение требований, предписанных дорожными знаками или разметкой проезжей части дороги': 500,\n",
    "    'Статья 12.16 часть 2 Поворот налево или разворот в нарушение требований, предписанных дорожными знаками или разметкой проезжей части дороги': 1000,\n",
    "    'Статья 12.17  часть 1.1 и 1.2. движение транспортных средств по полосе для маршрутных транспортных средств или остановка на указанной полосе в нарушение Правил дорожного движения ': 1500,\n",
    "    'Статья 12.12 часть 2 1. невыполнение требования ПДД об остановке перед стоп-линией, обозначенной дорожными знаками или разметкой проезжей части дороги, при запрещающем сигнале светофора или запрещающем жесте регулировщика': 800,\n",
    "    'Статья 12.15 часть 4 Выезд в нарушение правил дорожного движения на полосу, предназначенную для встречного движения, при объезде препятствия, либо на трамвайные пути встречного направления, за исключением случаев, предусмотренных частью 3 настоящей статьи': 5000\n",
    "}\n",
    "\n",
    "# ======== Загрузка моделей ======== #\n",
    "# Устройство (CPU или GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Используется устройство: {device}\")\n",
    "\n",
    "# Загрузка XCLIP модели и процессора\n",
    "processor = XCLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "model = XCLIPModel.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "model.eval()  # Переводим модель в режим оценки\n",
    "\n",
    "# Загрузка модели классификатора и весов\n",
    "classifier_model = EmbeddingClassifier(model.config.projection_dim, NUM_CLASSES)\n",
    "classifier_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "classifier_model.to(device)\n",
    "classifier_model.eval()\n",
    "\n",
    "# Получение mean и std для нормализации изображений\n",
    "try:\n",
    "    image_mean = processor.image_processor.image_mean\n",
    "    image_std = processor.image_processor.image_std\n",
    "except AttributeError:\n",
    "    image_mean = processor.feature_extractor.image_mean\n",
    "    image_std = processor.feature_extractor.image_std\n",
    "\n",
    "# Определение видео трансформаций\n",
    "video_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_mean, std=image_std)\n",
    "])\n",
    "\n",
    "# ======== Класс для предварительной обработки ======== #\n",
    "class Preprocessor:\n",
    "    def __init__(self, yolo_custom_path, segformer_model_path):\n",
    "        self.device = device\n",
    "        # Загрузка кастомной модели YOLOv5\n",
    "        self.custom_model = torch.hub.load(\n",
    "            'ultralytics/yolov5', 'custom', path=yolo_custom_path, force_reload=True\n",
    "        ).to(self.device).eval()\n",
    "        # Загрузка предобученной модели YOLOv5\n",
    "        self.pretrained_model = torch.hub.load(\n",
    "            'ultralytics/yolov5', 'yolov5n', pretrained=True\n",
    "        ).to(self.device).eval()\n",
    "        # Загрузка модели SegFormer\n",
    "        self.segformer_model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            segformer_model_path\n",
    "        ).to(self.device).eval()\n",
    "        self.extractor = SegformerImageProcessor()\n",
    "        # Параметры\n",
    "        self.traffic_related_classes = [\"car\", \"bus\", \"truck\", \"motorcycle\", \"bicycle\"]\n",
    "        self.target_class_id = 2  # Целевой класс для SegFormer\n",
    "\n",
    "    def apply(self, frame):\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        # Преобразование кадра\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(rgb_frame)\n",
    "\n",
    "        # Получение результатов от моделей YOLOv5\n",
    "        results_pretrained = self.pretrained_model(img)\n",
    "        results_custom = self.custom_model(img)\n",
    "\n",
    "        # Объединение результатов\n",
    "        results_combined = pd.concat(\n",
    "            [results_pretrained.pandas().xyxy[0], results_custom.pandas().xyxy[0]],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "        # Обработка кадра моделью SegFormer\n",
    "        seg_map = self.predict_segformer(rgb_frame)\n",
    "\n",
    "        # Создание маски для затемнения\n",
    "        mask = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "        # Добавление результатов YOLOv5 в маску\n",
    "        for _, row in results_combined.iterrows():\n",
    "            if row[\"name\"] in self.traffic_related_classes or row[\"confidence\"] > 0.25:\n",
    "                x1 = int(max(0, row[\"xmin\"]))\n",
    "                y1 = int(max(0, row[\"ymin\"]))\n",
    "                x2 = int(min(width - 1, row[\"xmax\"]))\n",
    "                y2 = int(min(height - 1, row[\"ymax\"]))\n",
    "                mask[y1:y2, x1:x2] = 255  # Область, которую не затемняем\n",
    "\n",
    "        # Добавление результатов SegFormer в маску\n",
    "        if seg_map.shape != (height, width):\n",
    "            seg_map_resized = cv2.resize(seg_map, (width, height), interpolation=cv2.INTER_NEAREST)\n",
    "        else:\n",
    "            seg_map_resized = seg_map\n",
    "        seg_mask = np.where(seg_map_resized == self.target_class_id, 255, 0).astype(np.uint8)\n",
    "        mask = cv2.bitwise_or(mask, seg_mask)\n",
    "\n",
    "        # Создание итогового кадра с затемнением\n",
    "        alpha_mask = np.stack([mask, mask, mask], axis=-1)  # Создаем маску с 3 каналами\n",
    "        frame_darkened = (frame * 0.2).astype(np.uint8)\n",
    "        frame_result = np.where(alpha_mask == 255, frame, frame_darkened)\n",
    "\n",
    "        return frame_result\n",
    "\n",
    "    def predict_segformer(self, image):\n",
    "        inputs = self.extractor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.segformer_model(**inputs)\n",
    "        logits = outputs.logits  # [batch_size, num_classes, height, width]\n",
    "        segmentation = torch.argmax(logits, dim=1).squeeze(0)\n",
    "        return segmentation.cpu().numpy()\n",
    "\n",
    "# Инициализация препроцессора при необходимости\n",
    "if APPLY_PREPROCESSING:\n",
    "    preprocessor = Preprocessor(YOLO_CUSTOM_PATH, SEGFORMER_MODEL_PATH)\n",
    "else:\n",
    "    preprocessor = None\n",
    "\n",
    "# ======== Функции для обработки видео ======== #\n",
    "def extract_frames_from_video(\n",
    "    video_capture, start_time, end_time, num_frames=8, preprocessor=None\n",
    "):\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    start_frame = int(start_time * fps)\n",
    "    end_frame = int(end_time * fps)\n",
    "    total_frames = end_frame - start_frame\n",
    "\n",
    "    frame_indices = np.linspace(start_frame, end_frame - 1, num=num_frames, dtype=int)\n",
    "    frames = []\n",
    "\n",
    "    for frame_idx in frame_indices:\n",
    "        video_capture.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        success, frame = video_capture.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Применение предварительной обработки\n",
    "        if preprocessor is not None:\n",
    "            frame = preprocessor.apply(frame)\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_pil = Image.fromarray(frame_rgb)\n",
    "\n",
    "        # Используем заданные трансформации\n",
    "        frame_tensor = video_transform(frame_pil)\n",
    "        frames.append(frame_tensor)\n",
    "\n",
    "    # Повторяем последний кадр, если кадров меньше, чем num_frames\n",
    "    while len(frames) < num_frames:\n",
    "        frames.append(frames[-1].clone() if len(frames) > 0 else torch.zeros(3, 224, 224))\n",
    "\n",
    "    # Преобразуем список тензоров в один тензор и перемещаем на устройство\n",
    "    video_frames_tensor = torch.stack(frames).to(device)\n",
    "\n",
    "    return video_frames_tensor\n",
    "\n",
    "# ======== Функция для предсказания класса сегмента ======== #\n",
    "def predict_segment_class(video_capture, start_time, end_time, preprocessor=None):\n",
    "    # Извлекаем кадры из сегмента\n",
    "    video_frames_tensor = extract_frames_from_video(\n",
    "        video_capture, start_time, end_time, num_frames=8, preprocessor=preprocessor\n",
    "    )\n",
    "    video_frames_tensor = video_frames_tensor.unsqueeze(0)  # Добавляем размерность batch\n",
    "\n",
    "    # Генерируем фиктивный текстовый ввод\n",
    "    text_inputs = processor(\n",
    "        text=[\"\"],  # Пустой текст, так как мы используем только видеоэмбеддинги\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77\n",
    "    )\n",
    "    input_ids = text_inputs['input_ids'].to(device)\n",
    "    attention_mask = text_inputs['attention_mask'].to(device)\n",
    "\n",
    "    # Получаем видеоэмбеддинги из модели XCLIP\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            pixel_values=video_frames_tensor,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        video_embeds = outputs.video_embeds  # [batch_size, projection_dim]\n",
    "\n",
    "    # Передаем эмбеддинги в классификатор\n",
    "    with torch.no_grad():\n",
    "        logits = classifier_model(video_embeds)\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
    "        predicted_class = LABEL_LIST[predicted_class_idx]\n",
    "        confidence = probabilities[0, predicted_class_idx].item()\n",
    "\n",
    "    return predicted_class, confidence\n",
    "\n",
    "# ======== Основная функция для обработки видео ======== #\n",
    "def process_video(video_path, preprocessor=None):\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    if not video_capture.isOpened():\n",
    "        print(f\"Не удалось открыть видео: {video_path}\")\n",
    "        return []\n",
    "\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    duration = total_frames / fps\n",
    "    print(f\"Видео {os.path.basename(video_path)} длительностью {duration:.2f} секунд, FPS: {fps}\")\n",
    "\n",
    "    segment_duration = 10  # Продолжительность сегмента в секундах\n",
    "    predictions = []\n",
    "\n",
    "    num_segments = int(np.ceil(duration / segment_duration))\n",
    "    for i in range(num_segments):\n",
    "        start_time = i * segment_duration\n",
    "        end_time = min((i + 1) * segment_duration, duration)\n",
    "\n",
    "        predicted_class, confidence = predict_segment_class(\n",
    "            video_capture, start_time, end_time, preprocessor=preprocessor\n",
    "        )\n",
    "\n",
    "        # Вычисляем среднее время сегмента\n",
    "        violation_time = int((start_time + end_time) / 2)\n",
    "\n",
    "        # Получаем сумму штрафа\n",
    "        fine_amount = FINE_DICT.get(predicted_class, 0)\n",
    "\n",
    "        # Если нарушение отсутствует, не добавляем его в предсказания\n",
    "        if predicted_class == 'нарушений нет':\n",
    "            continue\n",
    "\n",
    "        print(f\"Сегмент {i+1}/{num_segments}, время: {start_time:.2f}-{end_time:.2f} сек, \"\n",
    "              f\"Класс: {predicted_class}, Доверие: {confidence:.4f}, Время нарушения: {violation_time} сек\")\n",
    "\n",
    "        predictions.append({\n",
    "            'номер видео': os.path.splitext(os.path.basename(video_path))[0],\n",
    "            'наименование нарушения': predicted_class,\n",
    "            'сумма штрафа, руб.': fine_amount,\n",
    "            'время нарушения (в секундах)': float(violation_time)\n",
    "        })\n",
    "\n",
    "    video_capture.release()\n",
    "    return predictions\n",
    "\n",
    "# ======== Обработка всех видео и формирование сабмита ======== #\n",
    "def create_submission(video_paths, output_csv='submission.csv', preprocessor=None):\n",
    "    all_predictions = []\n",
    "\n",
    "    for video_path in video_paths:\n",
    "        predictions = process_video(video_path, preprocessor=preprocessor)\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    submission_df = pd.DataFrame(all_predictions)\n",
    "    submission_df.sort_values(by=['номер видео', 'время нарушения (в секундах)'], inplace=True)\n",
    "    submission_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Сабмит сохранен в файл {output_csv}\")\n",
    "\n",
    "# ======== Запуск скрипта ======== #\n",
    "if __name__ == \"__main__\":\n",
    "    # Список видеофайлов для обработки\n",
    "    video_files = [\n",
    "        os.path.join(video_folder, filename)\n",
    "        for filename in os.listdir(video_folder)\n",
    "        if filename.endswith(('.mp4', '.mov', '.avi'))  # Замените на нужные расширения файлов\n",
    "    ]\n",
    "\n",
    "    # Обработка видео и создание сабмита\n",
    "    create_submission(video_files, output_csv=output_csv, preprocessor=preprocessor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
